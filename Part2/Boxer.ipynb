{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install ftfy regex tqdm\n! pip install git+https://github.com/openai/CLIP.git","metadata":{"execution":{"iopub.status.idle":"2023-04-21T17:43:11.377258Z","shell.execute_reply.started":"2023-04-21T17:42:49.718236Z","shell.execute_reply":"2023-04-21T17:43:11.375983Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport clip\nimport os\nimport shutil\nimport random\nfrom tqdm.notebook import tqdm\nfrom pkg_resources import packaging\n\n# Define basic and modified prompts\nbasic_prompt = \"a photo of a {}\"\nmodified_prompt = \"a photo of a {}, a type of pet\"\n\ntemplates1 = [basic_prompt] \ntemplates2 = [modified_prompt]","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:43:11.379758Z","iopub.execute_input":"2023-04-21T17:43:11.380486Z","iopub.status.idle":"2023-04-21T17:43:11.388002Z","shell.execute_reply.started":"2023-04-21T17:43:11.380439Z","shell.execute_reply":"2023-04-21T17:43:11.386403Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def split_images(data_dir):\n    images = \"/kaggle/working/images\"\n    if(not os.path.exists(images)):\n        os.makedirs(images)\n    for filename in os.listdir(data_dir):\n        class_name = \"_\".join(list(filename.split(\"_\"))[:-1])\n        class_dir = os.path.join(images, class_name)\n        src_dir = os.path.join(data_dir,filename)\n        if(not os.path.exists(class_dir)):\n            os.makedirs(class_dir)\n        shutil.copy(src_dir,class_dir)\n    \n    shutil.make_archive(os.path.join(\"/kaggle/working/\",\"oxford_dataset_iiit-pet_splitted\"), 'zip', images)\n\n\n# Create zeroshot weights\ndef zeroshot_classifier(classnames, templates):\n    with torch.no_grad():\n        zeroshot_weights = []\n        for classname in tqdm(classnames):\n            texts = [template.format(classname) for template in templates] #format with class\n            texts = clip.tokenize(texts).cuda() #tokenize\n            class_embeddings = model.encode_text(texts) #embed with text encoder\n            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n            class_embedding = class_embeddings.mean(dim=0)\n            class_embedding /= class_embedding.norm()\n            zeroshot_weights.append(class_embedding)\n        zeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()\n    return zeroshot_weights\n\ndef accuracy(output, target, boxer_label,topk = (1,)):\n    true_positive = 0\n    false_positive = 0\n    true_negative = 0\n    false_negative = 0\n    pred = (output.topk(max(topk), 1, True, True)[1].t()).flatten() == boxer_label\n    target = (target == boxer_label).flatten()\n    for i in range(pred.shape[0]):\n        if(target[i] == True):\n            if(pred[i] == target[i]):\n                true_positive += 1\n            else:\n                false_negative += 1\n        else:\n            if(pred[i] == target[i]):\n                true_negative += 1\n            else:\n                false_positive += 1\n            \n    return (true_positive,false_positive,true_negative,false_negative)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:43:46.997002Z","iopub.execute_input":"2023-04-21T17:43:46.997504Z","iopub.status.idle":"2023-04-21T17:43:47.012012Z","shell.execute_reply.started":"2023-04-21T17:43:46.997462Z","shell.execute_reply":"2023-04-21T17:43:47.010756Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Load pre-trained CLIP model\nclip.available_models()\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\")\ninput_resolution = model.visual.input_resolution\ncontext_length = model.context_length\nvocab_size = model.vocab_size\n\nprint(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\nprint(\"Input resolution:\", input_resolution)\nprint(\"Context length:\", context_length)\nprint(\"Vocab size:\", vocab_size)","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:43:51.732383Z","iopub.execute_input":"2023-04-21T17:43:51.733066Z","iopub.status.idle":"2023-04-21T17:43:56.221834Z","shell.execute_reply.started":"2023-04-21T17:43:51.733030Z","shell.execute_reply":"2023-04-21T17:43:56.220605Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Model parameters: 151,277,313\nInput resolution: 224\nContext length: 77\nVocab size: 49408\n","output_type":"stream"}]},{"cell_type":"code","source":"# split_images(data_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for small dataset\ndata_dir = \"/kaggle/working/\"\ndata_set = datasets.OxfordIIITPet(data_dir, transform=preprocess,download = True)\nloader = torch.utils.data.DataLoader(data_set, batch_size=32, num_workers=2)\nclasses = data_set.classes\nboxer_label = classes.index('Boxer')\nzeroshot_weights = zeroshot_classifier(classes, templates1)\nmodified_zeroshot_weights = zeroshot_classifier(classes, templates2)\n\n\nwith torch.no_grad():\n    tp,fp,tn,fn = 0,0,0,0\n    mtp,mfp,mtn,mfn = 0,0,0,0\n    for i, (images, target) in enumerate(tqdm(loader)):\n        images = images.cuda()\n        target = target.cuda()\n        \n        # predict\n        image_features = model.encode_image(images)\n        image_features /= image_features.norm(dim=-1, keepdim=True)\n        logits = 100. * image_features @ zeroshot_weights\n        modified_logits = 100. * image_features @ modified_zeroshot_weights\n\n        # measure accuracy\n        tp1,fp1,tn1,fn1 = accuracy(logits, target,boxer_label)\n        tp += tp1\n        fp += fp1\n        tn += tn1\n        fn += fn1\n        \n        mtp1,mfp1,mtn1,mfn1 = accuracy(modified_logits, target,boxer_label)\n        mtp += mtp1\n        mfp += mfp1\n        mtn += mtn1\n        mfn += mfn1\n\n\nbp = (tp/(tp + fp))\nbr = (tp/(tp + fn))\nbacc = (tp + tn)/(tp + tn + fp + fn)\nbf1 = (2*bp*br)/(bp + br)\nprint(f\"Basic_prompt: \\n  precision-{bp} \\n  recall-{br} \\n  accuracy-{bacc} \\n  f1score-{bf1} \\n \\n\")\nmp = (mtp/(mtp + mfp))\nmr = (mtp/(mtp + mfn))\nmacc = (mtp + mtn)/(mtp + mtn + mfp + mfn)\nmf1 = (2*mp*mr)/(mp + mr)\nprint(f\"Modified_prompt: \\n  precision-{mp} \\n  recall-{mr} \\n  accuracy-{macc}  \\n  f1score-{mf1}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:47:40.891939Z","iopub.execute_input":"2023-04-21T17:47:40.892381Z","iopub.status.idle":"2023-04-21T17:48:10.314427Z","shell.execute_reply.started":"2023-04-21T17:47:40.892343Z","shell.execute_reply":"2023-04-21T17:48:10.313017Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/37 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3f69d4d5f894ebc901ae4b16f74a339"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/37 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c60a4e7ddcd24beda8c9bb1372ff911e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/115 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eebf7cf2f9984eefbb8872c07354c895"}},"metadata":{}},{"name":"stdout","text":"Basic_prompt: \n  precision-0.9047619047619048 \n  recall-0.38 \n  accuracy-0.9820652173913044 \n  f1score-0.5352112676056339 \n \n\nModified_prompt: \n  precision-0.8333333333333334 \n  recall-0.85 \n  accuracy-0.991304347826087  \n  f1score-0.8415841584158417\n","output_type":"stream"}]},{"cell_type":"code","source":"#for big dataset\ndata_dir = \"/kaggle/input/oxford-dataset-iiit-pet-splitted\"\ndata_set = datasets.ImageFolder(data_dir, transform=preprocess)\n\nloader = torch.utils.data.DataLoader(data_set, batch_size=32, num_workers=2)\nclasses = data_set.classes\nboxer_label = classes.index('boxer')\nzeroshot_weights = zeroshot_classifier(classes, templates1)\nmodified_zeroshot_weights = zeroshot_classifier(classes, templates2)\n\n\nwith torch.no_grad():\n    tp,fp,tn,fn = 0,0,0,0\n    mtp,mfp,mtn,mfn = 0,0,0,0\n    for i, (images, target) in enumerate(tqdm(loader)):\n        images = images.cuda()\n        target = target.cuda()\n        \n        # predict\n        image_features = model.encode_image(images)\n        image_features /= image_features.norm(dim=-1, keepdim=True)\n        logits = 100. * image_features @ zeroshot_weights\n        modified_logits = 100. * image_features @ modified_zeroshot_weights\n\n        # measure accuracy\n        tp1,fp1,tn1,fn1 = accuracy(logits, target,boxer_label)\n        tp += tp1\n        fp += fp1\n        tn += tn1\n        fn += fn1\n        \n        mtp1,mfp1,mtn1,mfn1 = accuracy(modified_logits, target,boxer_label)\n        mtp += mtp1\n        mfp += mfp1\n        mtn += mtn1\n        mfn += mfn1\n\n\nbp = (tp/(tp + fp))\nbr = (tp/(tp + fn))\nbacc = (tp + tn)/(tp + tn + fp + fn)\nbf1 = (2*bp*br)/(bp + br)\nprint(f\"Basic_prompt: \\n  precision-{bp} \\n  recall-{br} \\n  accuracy-{bacc} \\n  f1score-{bf1} \\n \\n\")\nmp = (mtp/(mtp + mfp))\nmr = (mtp/(mtp + mfn))\nmacc = (mtp + mtn)/(mtp + mtn + mfp + mfn)\nmf1 = (2*mp*mr)/(mp + mr)\nprint(f\"Modified_prompt: \\n  precision-{mp} \\n  recall-{mr} \\n  accuracy-{macc}  \\n  f1score-{mf1}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:48:27.660290Z","iopub.execute_input":"2023-04-21T17:48:27.660900Z","iopub.status.idle":"2023-04-21T17:49:38.701343Z","shell.execute_reply.started":"2023-04-21T17:48:27.660852Z","shell.execute_reply":"2023-04-21T17:49:38.700206Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/37 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23999c1237de45a997d8abdd747690ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/37 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af26d472382844bab1170a7de514cfec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/231 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5722ad4178d345419cbc4b782fae3a6f"}},"metadata":{}},{"name":"stdout","text":"Basic_prompt: \n  precision-0.8880597014925373 \n  recall-0.595 \n  accuracy-0.9870094722598105 \n  f1score-0.7125748502994012 \n \n\nModified_prompt: \n  precision-0.8505154639175257 \n  recall-0.825 \n  accuracy-0.9913396481732071  \n  f1score-0.8375634517766498\n","output_type":"stream"}]}]}