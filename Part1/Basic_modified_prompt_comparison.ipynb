{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install ftfy regex tqdm\n! pip install git+https://github.com/openai/CLIP.git","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:53:07.790127Z","iopub.execute_input":"2023-04-21T17:53:07.790835Z","iopub.status.idle":"2023-04-21T17:53:29.874941Z","shell.execute_reply.started":"2023-04-21T17:53:07.790798Z","shell.execute_reply":"2023-04-21T17:53:29.873656Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Requirement already satisfied: ftfy in /opt/conda/lib/python3.7/site-packages (6.1.1)\nRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (2021.11.10)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.64.1)\nRequirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from ftfy) (0.2.6)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-xarog4ui\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-xarog4ui\n  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: ftfy in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (6.1.1)\nRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (2021.11.10)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (4.64.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (1.13.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (0.14.0)\nRequirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from ftfy->clip==1.0) (0.2.6)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->clip==1.0) (4.4.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision->clip==1.0) (2.28.2)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->clip==1.0) (9.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->clip==1.0) (1.21.6)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->clip==1.0) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->clip==1.0) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->clip==1.0) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->clip==1.0) (1.26.14)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport clip\nimport os\nimport shutil\nimport random\nfrom tqdm.notebook import tqdm\nfrom pkg_resources import packaging\n\n# Define basic and modified prompts\nbasic_prompt = \"a photo of a {}\"\nmodified_prompt = \"a photo of a {}, a type of pet\"\n\ntemplates1 = [basic_prompt] \ntemplates2 = [modified_prompt]","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:53:38.981853Z","iopub.execute_input":"2023-04-21T17:53:38.982249Z","iopub.status.idle":"2023-04-21T17:53:38.990405Z","shell.execute_reply.started":"2023-04-21T17:53:38.982214Z","shell.execute_reply":"2023-04-21T17:53:38.987917Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def split_images(data_dir):\n    images = \"/kaggle/working/images\"\n    if(not os.path.exists(images)):\n        os.makedirs(images)\n    for filename in os.listdir(data_dir):\n        class_name = \"_\".join(list(filename.split(\"_\"))[:-1])\n        class_dir = os.path.join(images, class_name)\n        src_dir = os.path.join(data_dir,filename)\n        if(not os.path.exists(class_dir)):\n            os.makedirs(class_dir)\n        shutil.copy(src_dir,class_dir)\n    \n    shutil.make_archive(os.path.join(\"/kaggle/working/\",\"oxford_dataset_iiit-pet_splitted\"), 'zip', images)\n\n\n# Create zeroshot weights\ndef zeroshot_classifier(classnames, templates):\n    with torch.no_grad():\n        zeroshot_weights = []\n        for classname in tqdm(classnames):\n            texts = [template.format(classname) for template in templates] #format with class\n            texts = clip.tokenize(texts).cuda() #tokenize\n            class_embeddings = model.encode_text(texts) #embed with text encoder\n            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n            class_embedding = class_embeddings.mean(dim=0)\n            class_embedding /= class_embedding.norm()\n            zeroshot_weights.append(class_embedding)\n        zeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()\n    return zeroshot_weights\n\ndef accuracy(output, target, topk=(1,)):\n    pred = output.topk(max(topk), 1, True, True)[1].t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n    return [float(correct[:k].reshape(-1).float().sum(0, keepdim=True).cpu().numpy()) for k in topk]\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:53:42.036026Z","iopub.execute_input":"2023-04-21T17:53:42.036743Z","iopub.status.idle":"2023-04-21T17:53:42.050658Z","shell.execute_reply.started":"2023-04-21T17:53:42.036703Z","shell.execute_reply":"2023-04-21T17:53:42.049367Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Load pre-trained CLIP model\nclip.available_models()\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\")\ninput_resolution = model.visual.input_resolution\ncontext_length = model.context_length\nvocab_size = model.vocab_size\n\nprint(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\nprint(\"Input resolution:\", input_resolution)\nprint(\"Context length:\", context_length)\nprint(\"Vocab size:\", vocab_size)","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:53:46.959169Z","iopub.execute_input":"2023-04-21T17:53:46.959943Z","iopub.status.idle":"2023-04-21T17:53:54.964379Z","shell.execute_reply.started":"2023-04-21T17:53:46.959903Z","shell.execute_reply":"2023-04-21T17:53:54.963264Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Model parameters: 151,277,313\nInput resolution: 224\nContext length: 77\nVocab size: 49408\n","output_type":"stream"}]},{"cell_type":"code","source":"# split_images(data_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#smaller dataset\ndata_dir = \"/kaggle/working/\"\ndata_set = datasets.OxfordIIITPet(data_dir, transform=preprocess,download = True)\nloader = torch.utils.data.DataLoader(data_set, batch_size=32, num_workers=2)\nclasses = data_set.classes\nzeroshot_weights = zeroshot_classifier(classes, templates1)\nmodified_zeroshot_weights = zeroshot_classifier(classes, templates2)\n\n\nwith torch.no_grad():\n    top1, top5, n = 0., 0., 0.\n    modified_top1,modified_top5 = 0. , 0.\n    for i, (images, target) in enumerate(tqdm(loader)):\n        images = images.cuda()\n        target = target.cuda()\n        \n        # predict\n        image_features = model.encode_image(images)\n        image_features /= image_features.norm(dim=-1, keepdim=True)\n        logits = 100. * image_features @ zeroshot_weights\n        modified_logits = 100. * image_features @ modified_zeroshot_weights\n\n        # measure accuracy\n        acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n        top1 += acc1\n        top5 += acc5\n        \n        modified_acc1, modified_acc5 = accuracy(modified_logits, target, topk=(1, 5))\n        modified_top1 += modified_acc1\n        modified_top5 += modified_acc5\n        \n        n += images.size(0)\n\ntop1 = (top1 / n) * 100\ntop5 = (top5 / n) * 100 \n\nmodified_top1 = (modified_top1 / n) * 100\nmodified_top5 = (modified_top5 / n) * 100 \n\nprint(f\"Top-1 basic-accuracy: {top1:.2f}\")\nprint(f\"Top-5 basic-accuracy: {top5:.2f}\")\n\nprint(f\"Top-1 modified-accuracy: {modified_top1:.2f}\")\nprint(f\"Top-5 modified-accuracy: {modified_top5:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:55:06.509726Z","iopub.execute_input":"2023-04-21T17:55:06.510572Z","iopub.status.idle":"2023-04-21T17:55:35.241118Z","shell.execute_reply.started":"2023-04-21T17:55:06.510533Z","shell.execute_reply":"2023-04-21T17:55:35.240006Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/37 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17faa23a6393434580b5f2fcf15c2558"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/37 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c26665491bc4159b7c6a4fe92e40f66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/115 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ec3076bac8440488b694e08f5b708bd"}},"metadata":{}},{"name":"stdout","text":"Top-1 basic-accuracy: 80.57\nTop-5 basic-accuracy: 96.60\nTop-1 modified-accuracy: 85.49\nTop-5 modified-accuracy: 99.13\n","output_type":"stream"}]},{"cell_type":"code","source":"#bigger dataset\ndata_dir = \"/kaggle/input/oxford-dataset-iiit-pet-splitted\"\ndata_set = datasets.ImageFolder(data_dir, transform=preprocess)\nloader = torch.utils.data.DataLoader(data_set, batch_size=32, num_workers=2)\nclasses = data_set.classes\nzeroshot_weights = zeroshot_classifier(classes, templates1)\nmodified_zeroshot_weights = zeroshot_classifier(classes, templates2)\n\n\nwith torch.no_grad():\n    top1, top5, n = 0., 0., 0.\n    modified_top1,modified_top5 = 0. , 0.\n    for i, (images, target) in enumerate(tqdm(loader)):\n        images = images.cuda()\n        target = target.cuda()\n        \n        # predict\n        image_features = model.encode_image(images)\n        image_features /= image_features.norm(dim=-1, keepdim=True)\n        logits = 100. * image_features @ zeroshot_weights\n        modified_logits = 100. * image_features @ modified_zeroshot_weights\n\n        # measure accuracy\n        acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n        top1 += acc1\n        top5 += acc5\n        \n        modified_acc1, modified_acc5 = accuracy(modified_logits, target, topk=(1, 5))\n        modified_top1 += modified_acc1\n        modified_top5 += modified_acc5\n        \n        n += images.size(0)\n\ntop1 = (top1 / n) * 100\ntop5 = (top5 / n) * 100 \n\nmodified_top1 = (modified_top1 / n) * 100\nmodified_top5 = (modified_top5 / n) * 100 \n\nprint(f\"Top-1 basic-accuracy: {top1:.2f}\")\nprint(f\"Top-5 basic-accuracy: {top5:.2f}\")\n\nprint(f\"Top-1 modified-accuracy: {modified_top1:.2f}\")\nprint(f\"Top-5 modified-accuracy: {modified_top5:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:55:41.584067Z","iopub.execute_input":"2023-04-21T17:55:41.584851Z","iopub.status.idle":"2023-04-21T17:56:41.022178Z","shell.execute_reply.started":"2023-04-21T17:55:41.584809Z","shell.execute_reply":"2023-04-21T17:56:41.020812Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/37 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27e98fd9bf224401be282bc6ab33e88f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/37 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58287759fce24e98b33106a02347b1d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/231 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84400d2cc5854e5597bf4fd153594e3a"}},"metadata":{}},{"name":"stdout","text":"Top-1 basic-accuracy: 77.93\nTop-5 basic-accuracy: 95.01\nTop-1 modified-accuracy: 82.21\nTop-5 modified-accuracy: 97.24\n","output_type":"stream"}]}]}